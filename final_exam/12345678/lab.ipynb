{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam - Data Analytics with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "1. Do not rename the placeholder variables\n",
    "2. The required results / outputs must be assigned to corresponding placeholder variables. Otherwise, no points will be counted.\n",
    "3. Variables prefixed with `df_` have DataFrames datatype. You must return or assign DataFrames value to those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init codes - No changes required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Init Spark session using the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# the following Spark session creation contains jars which support writing to Delta format\n",
    "# note: checkout this link to see which version of Delta is compatible with your version of Pyspark: https://docs.delta.io/latest/releases.html\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final Exam Application\")  \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")    \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")    \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"final_exam/streaming_checkpoints\")    \\\n",
    "\t.config('spark.jars.packages', 'io.delta:delta-spark_2.12:3.2.0,graphframes:graphframes:0.8.4-spark3.5-s_2.12')\t\\\n",
    "\t.config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\t\\\n",
    "\t.config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\t\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir('final_exam/spark_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"final_exam/12345678/data/input\"\n",
    "input_streams_path = \"final_exam/12345678/data/input/streams\"\n",
    "output_path = \"final_exam/12345678/data/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering, Preprocessing, and Cleansing with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: DataFrame Creation\n",
    "- Create a DataFrame from a CSV file containing user data. \n",
    "- The CSV file is located under `input_path`.\n",
    "\t- User data: \"user_data.csv\"\n",
    "\t- Sales data: \"sales_data.csv\"\n",
    "\t- Transactions data: \"transactions_data.csv\"\n",
    "\t- Orders data: \"orders_data.csv\"\n",
    "\t- Customers data: \"customer_data.csv\"\n",
    "\t- and others ...\n",
    "\n",
    "- Ensure that the DataFrame includes the header from the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------------------+\n",
      "|userId|        name|               email|\n",
      "+------+------------+--------------------+\n",
      "|     1|     JohnDoe| johndoe@example.com|\n",
      "|     2|   JaneSmith|janesmith@example...|\n",
      "|     3|    BobBrown|bobbrown@example.com|\n",
      "|     4|AliceJohnson|alicejohnson@exam...|\n",
      "|     5|CharlieDavis|charliedavis@exam...|\n",
      "|     6|  DianaMoore|dianamoore@exampl...|\n",
      "|     7| EthanTaylor|ethantaylor@examp...|\n",
      "|     8| GraceWilson|gracewilson@examp...|\n",
      "|     9|HankAnderson|hankanderson@exam...|\n",
      "|    10| IvyThompson|ivythompson@examp...|\n",
      "+------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "user_data_file_path = f\"{input_path}/user_data.csv\"\n",
    "sales_data_file_path = f\"{input_path}/sales_data.csv\"\n",
    "transaction_data_file_path = f\"{input_path}/transactions.csv\"\n",
    "orders_data_file_path = f\"{input_path}/orders.csv\"\n",
    "customer_data_file_path = f\"{input_path}/customer_data.csv\"\n",
    "# and others ...\n",
    "\n",
    "df_user = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(user_data_file_path)\n",
    "df_sales = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(sales_data_file_path)\n",
    "df_transactions = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(transaction_data_file_path)\n",
    "df_orders = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(orders_data_file_path)\n",
    "df_customer_data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(customer_data_file_path)\n",
    "# and others ...\n",
    "\n",
    "# Show the DataFrames\n",
    "df_user.show(10)\n",
    "# and others ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Data Cleaning\n",
    "In `df_sales`, some entries in the `price` column are null. \n",
    "Your task is to remove rows where the `price` is null and display the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+------+-----+--------+--------+----------------+-------------------+\n",
      "|OrderID|      Date|CustomerID|ItemID|price|quantity|s_market|f_original_price|s_original_currency|\n",
      "+-------+----------+----------+------+-----+--------+--------+----------------+-------------------+\n",
      "|OR12345|15.10.2015|  CUS54343|IT8798|   45|       1|      HU|        14234.76|                HUF|\n",
      "|OR12345|15.10.2015|  CUS54343|IT2235|   30|       1|      HU|         9546.23|                HUF|\n",
      "|OR12345|15.10.2015|  CUS54343|IT8840|   25|       1|      HU|         3452.45|                HUF|\n",
      "|OR12346|16.10.2015|  CUS54344|IT8799|   50|       2|      HU|         15000.0|                HUF|\n",
      "|OR12346|16.10.2015|  CUS54344|IT2240|   20|      -1|      HU|         -7500.0|                HUF|\n",
      "|OR12347|17.10.2015|  CUS54345|IT8800|  -30|      -1|      HU|         -9000.0|                HUF|\n",
      "|OR12348|18.10.2015|  CUS54346|IT8801|  -25|      -1|      HU|         -5000.0|                HUF|\n",
      "|OR12349|19.10.2015|  CUS54347|IT8802|  -15|      -1|      HU|         -3000.0|                HUF|\n",
      "|OR12350|20.10.2015|  CUS54348|IT8803|  -10|      -1|      HU|         -2000.0|                HUF|\n",
      "|OR12351|21.10.2015|  CUS54349|IT8804|   -5|      -1|      HU|         -1000.0|                HUF|\n",
      "+-------+----------+----------+------+-----+--------+--------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with null values in the price column\n",
    "df_cleaned_sales = df_sales.filter(df_sales.price.isNotNull())\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "df_cleaned_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Grouping and Aggregation\n",
    "Given a DataFrame `df_transactions` containing transaction records, group the data by `storeId` and calculate the total sales for each store. Display the results in descending order of total sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|storeId|sum(sales)|\n",
      "+-------+----------+\n",
      "|     25|       278|\n",
      "|   NULL|        85|\n",
      "|     26|        51|\n",
      "|      3|        48|\n",
      "|      2|        29|\n",
      "|     27|        10|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by storeId and calculate total sales\n",
    "df_total_sales = df_transactions.groupBy(\"storeId\").agg({\"sales\": \"sum\"}).orderBy(\"sum(sales)\", ascending=False)\n",
    "\n",
    "# Show the results\n",
    "df_total_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Handling Duplicate Records\n",
    "You have a DataFrame `df_orders` with potential duplicate entries based on the `orderId`. Write code to remove duplicates while keeping the first occurrence of each order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+\n",
      "|orderId|      date|customerId| itemId|\n",
      "+-------+----------+----------+-------+\n",
      "|OR10001|2024-01-01|  CUS10001|ITEM001|\n",
      "|OR10002|2024-01-02|  CUS10002|ITEM002|\n",
      "|OR10003|2024-01-03|  CUS10003|ITEM003|\n",
      "|OR10004|2024-01-04|  CUS10004|ITEM004|\n",
      "|OR10005|2024-01-05|  CUS10005|ITEM005|\n",
      "|OR10006|2024-01-06|  CUS10006|ITEM006|\n",
      "|OR10007|2024-01-07|  CUS10007|ITEM007|\n",
      "|OR10008|2024-01-08|  CUS10008|ITEM008|\n",
      "|OR10009|2024-01-09|  CUS10009|ITEM009|\n",
      "|OR10010|2024-01-10|  CUS10010|ITEM010|\n",
      "+-------+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate orders based on orderId\n",
    "df_unique_order = df_orders.dropDuplicates([\"orderId\"])\n",
    "\n",
    "# Show unique orders\n",
    "df_unique_order.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Complex Filtering\n",
    "Using a DataFrame `df_customer_data`, filter out customers who have not made any purchases in the last year. Assume there is a column `last_purchase_date`. Display only customer IDs and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|customerId|name|\n",
      "+----------+----+\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, datediff\n",
    "\n",
    "# Filter customers based on last purchase date\n",
    "df_active_customers = df_customer_data.filter(datediff(current_date(), df_customer_data.last_purchase_date) <= 365).select(\"customerId\", \"name\")\n",
    "\n",
    "# Show active customers\n",
    "df_active_customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Feature Selection\n",
    "You have a DataFrame `df_training_data` with multiple features. Select only the features feature1, feature2, and label for your model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|feature1|value|label|\n",
      "+--------+-----+-----+\n",
      "|     0.1| NULL|    1|\n",
      "|     0.2| NULL|    0|\n",
      "|    0.15| NULL|    1|\n",
      "|    0.25| NULL|    0|\n",
      "|    0.35| NULL|    1|\n",
      "|    0.45| NULL|    0|\n",
      "|    0.55| NULL|    1|\n",
      "|    0.65| NULL|    0|\n",
      "|    0.75| NULL|    1|\n",
      "|    0.85| NULL|    0|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data_file_path = f\"{input_path}/training_data.csv\"\n",
    "df_training_data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(training_data_file_path)\n",
    "df_training_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|feature1|label|\n",
      "+--------+-----+\n",
      "|     0.1|    1|\n",
      "|     0.2|    0|\n",
      "|    0.15|    1|\n",
      "|    0.25|    0|\n",
      "|    0.35|    1|\n",
      "|    0.45|    0|\n",
      "|    0.55|    1|\n",
      "|    0.65|    0|\n",
      "|    0.75|    1|\n",
      "|    0.85|    0|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select specific features for model training\n",
    "df_selected_features = df_training_data.select(\"feature1\", \"label\")\n",
    "\n",
    "# Show selected features\n",
    "df_selected_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Creating New Features\n",
    "From a DataFrame `df_employee`, create a new feature called years_of_experience by subtracting the start_year from the current year. Display the updated DataFrame with this new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|employeeId|start_year|\n",
      "+----------+----------+\n",
      "|    EMP001|      2018|\n",
      "|    EMP002|      2019|\n",
      "|    EMP003|      2020|\n",
      "|    EMP004|      2017|\n",
      "|    EMP005|      2016|\n",
      "|    EMP006|      2018|\n",
      "|    EMP007|      2019|\n",
      "|    EMP008|      2020|\n",
      "|    EMP009|      2021|\n",
      "|    EMP010|      2022|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_data_file_path = f\"{input_path}/employee_data.csv\"\n",
    "df_employee = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(employee_data_file_path)\n",
    "df_employee.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|employeeId|start_year|years_of_experience|\n",
      "+----------+----------+-------------------+\n",
      "|    EMP001|      2018|                  6|\n",
      "|    EMP002|      2019|                  5|\n",
      "|    EMP003|      2020|                  4|\n",
      "|    EMP004|      2017|                  7|\n",
      "|    EMP005|      2016|                  8|\n",
      "|    EMP006|      2018|                  6|\n",
      "|    EMP007|      2019|                  5|\n",
      "|    EMP008|      2020|                  4|\n",
      "|    EMP009|      2021|                  3|\n",
      "|    EMP010|      2022|                  2|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, current_date, col\n",
    "\n",
    "\n",
    "# Add years_of_experience feature\n",
    "df_employee_with_experience = df_employee.withColumn(\"years_of_experience\", year(current_date()) - col(\"start_year\"))\n",
    "\n",
    "# Show updated employee data\n",
    "df_employee_with_experience.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: One-Hot Encoding\n",
    "You have a categorical feature category in your DataFrame `df_products`. Perform one-hot encoding on this feature and display the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|productId|      name|    categories|\n",
      "+---------+----------+--------------+\n",
      "|     P001|    Laptop|   Electronics|\n",
      "|     P002|Smartphone|   Electronics|\n",
      "|     P003|    Tablet|   Electronics|\n",
      "|     P004|     Shoes|      Footwear|\n",
      "|     P005|     Shirt|      Clothing|\n",
      "|     P006|     Pants|      Clothing|\n",
      "|     P007|       Hat|   Accessories|\n",
      "|     P008|     Watch|   Accessories|\n",
      "|     P009|  Backpack|Bags & Luggage|\n",
      "|     P010|Sunglasses|   Accessories|\n",
      "+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_data_file_path = f\"{input_path}/products.csv\"\n",
    "df_products = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(products_data_file_path)\n",
    "df_products.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+--------------+-------------+\n",
      "|productId|      name|    categories|category_index| category_ohe|\n",
      "+---------+----------+--------------+--------------+-------------+\n",
      "|     P001|    Laptop|   Electronics|           1.0|(4,[1],[1.0])|\n",
      "|     P002|Smartphone|   Electronics|           1.0|(4,[1],[1.0])|\n",
      "|     P003|    Tablet|   Electronics|           1.0|(4,[1],[1.0])|\n",
      "|     P004|     Shoes|      Footwear|           4.0|    (4,[],[])|\n",
      "|     P005|     Shirt|      Clothing|           2.0|(4,[2],[1.0])|\n",
      "|     P006|     Pants|      Clothing|           2.0|(4,[2],[1.0])|\n",
      "|     P007|       Hat|   Accessories|           0.0|(4,[0],[1.0])|\n",
      "|     P008|     Watch|   Accessories|           0.0|(4,[0],[1.0])|\n",
      "|     P009|  Backpack|Bags & Luggage|           3.0|(4,[3],[1.0])|\n",
      "|     P010|Sunglasses|   Accessories|           0.0|(4,[0],[1.0])|\n",
      "+---------+----------+--------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "\n",
    "# String indexer for category feature\n",
    "indexer = StringIndexer(inputCol=\"categories\", outputCol=\"category_index\")\n",
    "model = indexer.fit(df_products)\n",
    "df_indexed = model.transform(df_products)\n",
    "\n",
    "# One-hot encoding for indexed category feature\n",
    "encoder = OneHotEncoder(inputCols=[\"category_index\"], outputCols=[\"category_ohe\"])\n",
    "ohe_model = encoder.fit(df_indexed)\n",
    "df_encoded = ohe_model.transform(df_indexed)\n",
    "\n",
    "# Show encoded features\n",
    "df_encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Normalization of Features\n",
    "Normalize the feature amount in your DataFrame transactions. Use Min-Max scaling to achieve this. Display the transformed DataFrame with normalized values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+-------+---------+-----+\n",
      "|transactionId|predError|value|storeId|productId|sales|\n",
      "+-------------+---------+-----+-------+---------+-----+\n",
      "|            1|     NULL| NULL|     25|        1|   17|\n",
      "|            2|        6|    7|      2|        2|   29|\n",
      "|            3|        3| NULL|     25|        3|   93|\n",
      "|            4|     NULL| NULL|      3|        2|   48|\n",
      "|            5|     NULL| NULL|   NULL|        2|   85|\n",
      "|            6|        3|    2|     25|        2|   57|\n",
      "|            7|        4|    4|     25|        3|   56|\n",
      "|            8|        5| NULL|     25|        4|   55|\n",
      "|            9|        6| NULL|     26|        5|   51|\n",
      "|           10|        7|    8|     27|        6|   10|\n",
      "+-------------+---------+-----+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        scaled_sales|\n",
      "+--------------------+\n",
      "|[0.08433734939759...|\n",
      "|[0.22891566265060...|\n",
      "|               [1.0]|\n",
      "|[0.45783132530120...|\n",
      "|[0.9036144578313253]|\n",
      "|[0.5662650602409639]|\n",
      "|[0.5542168674698795]|\n",
      "|[0.5421686746987953]|\n",
      "|[0.49397590361445...|\n",
      "|               [0.0]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=[\"sales\"], outputCol=\"sales_vector\")\n",
    "vectorized_transactions = vector_assembler.transform(df_transactions)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"sales_vector\", outputCol=\"scaled_sales\")\n",
    "scaler_model = scaler.fit(vectorized_transactions)\n",
    "scaled_transactions = scaler_model.transform(vectorized_transactions)\n",
    "\n",
    "# Show scaled amounts\n",
    "scaled_transactions.select(\"scaled_sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Preparing Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|feature1|value|label|\n",
      "+--------+-----+-----+\n",
      "|     0.1| NULL|    1|\n",
      "|     0.2| NULL|    0|\n",
      "|    0.15| NULL|    1|\n",
      "|    0.25| NULL|    0|\n",
      "|    0.35| NULL|    1|\n",
      "|    0.45| NULL|    0|\n",
      "|    0.55| NULL|    1|\n",
      "|    0.65| NULL|    0|\n",
      "|    0.75| NULL|    1|\n",
      "|    0.85| NULL|    0|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data_file_path = f\"{input_path}/training_data.csv\"\n",
    "df_training_data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(training_data_file_path)\n",
    "df_training_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|feature1|value|label|\n",
      "+--------+-----+-----+\n",
      "|    0.12| NULL|  1.0|\n",
      "|    0.22| NULL|  0.0|\n",
      "|    0.18| NULL|  1.0|\n",
      "|    0.28| NULL|  0.0|\n",
      "|    0.38| NULL|  1.0|\n",
      "|    0.48| NULL|  0.0|\n",
      "|    0.58| NULL|  1.0|\n",
      "|    0.68| NULL|  0.0|\n",
      "|    0.78| NULL|  1.0|\n",
      "|    0.88| NULL|  0.0|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_file_path = f\"{input_path}/test_data.csv\"\n",
    "df_test_data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(test_data_file_path)\n",
    "df_test_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Models, Evaluate Models, Select Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Model Training\n",
    "Train a linear regression model using the DataFrame `df_training_data`, which includes features and labels. Display the coefficients of the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40257648953301134]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=[\"feature1\"], outputCol=\"features\")\n",
    "df_vectorized_training = vector_assembler.transform(df_training_data)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(df_vectorized_training)\n",
    "\n",
    "# Display model coefficients\n",
    "print(lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12: Model Evaluation Metrics\n",
    "Evaluate your trained linear regression model using RMSE (Root Mean Square Error) on test data. Display the RMSE value obtained from evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.49014973017959534\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "df_vectorized_test = vector_assembler.transform(df_test_data)\n",
    "\n",
    "# Evaluate model performance on test data\n",
    "predictions = lr_model.transform(df_vectorized_test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_value = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"RMSE: {rmse_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13: Cross-Validation for Hyperparameter Tuning\n",
    "Set up cross-validation for hyperparameter tuning of your linear regression model using a parameter grid for regularization parameter (regParam). Display best parameters after fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LinearRegression_511695632ff9', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LinearRegression_511695632ff9', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LinearRegression_511695632ff9', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35, Param(parent='LinearRegression_511695632ff9', name='featuresCol', doc='features column name.'): 'features', Param(parent='LinearRegression_511695632ff9', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LinearRegression_511695632ff9', name='labelCol', doc='label column name.'): 'label', Param(parent='LinearRegression_511695632ff9', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError', Param(parent='LinearRegression_511695632ff9', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LinearRegression_511695632ff9', name='maxIter', doc='max number of iterations (>= 0).'): 100, Param(parent='LinearRegression_511695632ff9', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LinearRegression_511695632ff9', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LinearRegression_511695632ff9', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto', Param(parent='LinearRegression_511695632ff9', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LinearRegression_511695632ff9', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n",
    "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "cv_model = crossval.fit(df_vectorized_training)\n",
    "best_model_params = cv_model.bestModel.extractParamMap()\n",
    "\n",
    "print(best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14: Model Selection Based on Tuned Hyperparameter\n",
    "Train the model with the `best_model_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = lr.fit(df_vectorized_training, params=best_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15: Predictions Using Selected Model\n",
    "Using your best-performing model from previous steps, make predictions on new data stored in new_data. Display predictions alongside actual labels if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+--------+\n",
      "|feature1|value|label|features|\n",
      "+--------+-----+-----+--------+\n",
      "|       0|     |     |   [0.0]|\n",
      "|       1|     |     |   [1.0]|\n",
      "|       2|     |     |   [2.0]|\n",
      "|       3|     |     |   [3.0]|\n",
      "|       4|     |     |   [4.0]|\n",
      "|       5|     |     |   [5.0]|\n",
      "|       6|     |     |   [6.0]|\n",
      "|       7|     |     |   [7.0]|\n",
      "|       8|     |     |   [8.0]|\n",
      "|       9|     |     |   [9.0]|\n",
      "+--------+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data_file_path = f\"{input_path}/new_data.csv\"\n",
    "df_new_data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(new_data_file_path)\n",
    "df_vectorized_new_data = vector_assembler.transform(df_new_data)\n",
    "df_vectorized_new_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          prediction|label|\n",
      "+--------------------+-----+\n",
      "|  0.6442565754159957|     |\n",
      "|  0.3087761674718197|     |\n",
      "|-0.02670424047235631|     |\n",
      "| -0.3621846484165323|     |\n",
      "| -0.6976650563607083|     |\n",
      "| -1.0331454643048843|     |\n",
      "| -1.3686258722490603|     |\n",
      "| -1.7041062801932365|     |\n",
      "| -2.0395866881374123|     |\n",
      "|  -2.375067096081588|     |\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_new_data = best_model.transform(df_vectorized_new_data)\n",
    "predictions_new_data.select(\"prediction\", \"label\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16: Basic Streaming Setup\n",
    "- Set up a basic streaming job that reads data from a CSV source and \n",
    "- Leave trigger mode as default (runs micro-batch as soon as it can)\n",
    "- Writes it to a memory table named `tbl_streaming_01`\n",
    "- Output mode is set to `update`\n",
    "- Print the results using Spark SQL\n",
    "- Stop the Streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create streaming context and read from csv source\n",
    "stream_path = \"final_exam/12345678/data/input/streams\"\n",
    "schema = 'timestamp timestamp, value double'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streaming_data = spark.readStream.format('csv').option('header','true').schema(schema).load(stream_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "This query does not support recovering from checkpoint location. Delete final_exam/streaming_checkpoints/tbl_streaming_01/offsets to start over.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mdf_streaming_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmemory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtbl_streaming_01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\streaming\\readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: This query does not support recovering from checkpoint location. Delete final_exam/streaming_checkpoints/tbl_streaming_01/offsets to start over."
     ]
    }
   ],
   "source": [
    "query = df_streaming_data.writeStream.format('memory').outputMode('update').queryName('tbl_streaming_01').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbl_streaming_01 = spark.sql(\"SELECT * FROM tbl_streaming_01\")\n",
    "\n",
    "df_tbl_streaming_01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17: Filtering Streaming Data\n",
    "- Filter incoming streaming data to include only messages with value >= 22.5. \n",
    "- Trigger mode is default.\n",
    "- Output mode is `append`\n",
    "- Write to an in-memory table named `tbl_streaming_02`\n",
    "- Print these filtered messages using Spark SQL.\n",
    "- Stop the Streaming query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredStreamDF = df_streaming_data.filter('value > 22.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_filtered = df_streaming_data.writeStream.format('memory').outputMode('append').queryName('tbl_streaming_02').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tbl_streaming_02 = spark.sql(\"SELECT * FROM tbl_streaming_02\")\n",
    "\n",
    "df_tbl_streaming_02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_filtered.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18: Aggregating Streaming Data\n",
    "- Aggregate streaming data to get the average value each 5 seconds and print results to console.\n",
    "- Trigger mode: `availableNow`\n",
    "- Output mode: `complete`\n",
    "- Output format: `console`\n",
    "- Stop the Streaming query after use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated_stream = df_streaming_data.groupBy(window('timestamp', '5 second')).avg('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_agg = df_aggregated_stream.writeStream \\\n",
    "\t.format(\"console\")\t\\\n",
    "\t.outputMode(\"complete\")\t\\\n",
    "\t.trigger(availableNow=True)\t\\\n",
    "\t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19: Writing Streaming Output to File Sink\n",
    "- Write streaming output of `filteredStreamDF` file sink in Parquet format.\n",
    "- Trigger mode is `availableNow`\n",
    "- Output mode is \"Append\"\n",
    "- Stop the query after use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_output_path = \"final_exam/12345678/data/output/streams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink = filteredStreamDF.writeStream.format('parquet').outputMode('append').trigger(availableNow=True).start(f'{stream_output_path}/parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 20: Read and verify the written stream data\n",
    "- Read to the data frame named `df_written_stream`\n",
    "- Show top 10 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_written_stream = spark.read.parquet(f'{stream_output_path}/parquet')\n",
    "df_written_stream.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analytics with GraphFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 21: Creating GraphFrames from DataFrames\n",
    "Create a GraphFrame using vertices and edges stored in two separate DataFrames named `vertices` and `edges`. Display basic information about the graph created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "v_path = 'final_exam/12345678/data/input/vertices.csv'\n",
    "e_path = 'final_exam/12345678/data/input/edges.csv'\n",
    "\n",
    "df_vertices = spark.read.csv(path=v_path, header=True, inferSchema=True)\n",
    "df_edges = spark.read.csv(path=e_path, header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphFrame(df_vertices, df_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.vertices.show())\n",
    "print(g.edges.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 22: Finding Shortest Paths in Graphs\n",
    "Using your GraphFrame g, find shortest paths from vertex ID 'V002' to all other vertices within a maximum path length of 3. Display results as paths found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = g.shortestPaths(landmarks=[\"V002\"])\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 23: PageRank Algorithm Implementation\n",
    "Implement PageRank algorithm on your GraphFrame to find important vertices based on their connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pagerank = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "results_pagerank.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 24: Connected Components Analysis\n",
    "Perform connected components analysis on your GraphFrame to identify clusters of connected vertices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cc = g.connectedComponents()\n",
    "result_cc.select(\"id\", \"component\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 25: Triangle Count in Graphs\n",
    "Count triangles formed by vertices in your GraphFrame using triangle counting functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_count_result = g.triangleCount()\n",
    "triangle_count_result.select(\"id\", \"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory final_exam/spark_checkpoint/* does not exist.\n",
      "Directory final_exam/streaming_checkpoints/ does not exist.\n"
     ]
    }
   ],
   "source": [
    "### Clean up Checkpoints & Outputs\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the path to the checkpoint directory\n",
    "spark_checkpoint_dir = \"final_exam/spark_checkpoint/\"\n",
    "stream_checkpoint_dir = \"final_exam/streaming_checkpoints/\"\n",
    "directories = [spark_checkpoint_dir, stream_checkpoint_dir]\n",
    "\n",
    "# Remove the checkpoint directory if it exists\n",
    "for dir in directories:\n",
    "\tif os.path.exists(dir):\n",
    "\t\tshutil.rmtree(dir)\n",
    "\t\tprint(f\"Directory {dir} cleaned up.\")\n",
    "\telse:\n",
    "\t\tprint(f\"Directory {dir} does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
