{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateless Processing\n",
    "Stateless processing is where each record is processed independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SparkSession with required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Structured Streaming Application\")  \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")    \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")    \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"notebook_data/w09/checkpoints/example_01\")    \\\n",
    "\t.config('spark.jars.packages', 'io.delta:delta-spark_2.12:3.2.0')\t\\\n",
    "\t.config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\t\\\n",
    "\t.config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\t\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read streaming data from a folder\n",
    "input_path = \"notebook_data/w09/data/input\"\n",
    "schema = \"id INT, name STRING, age INT, event_time timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Stream from CSV source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Stream to Console (for debugging) with filter\n",
    "Console Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stateless operation: Filter rows where age > 18\n",
    "# Console Sink supports all output modes: append / complete / update\n",
    "# change the outputMode to see the differences\n",
    "\n",
    "query = df.filter(col(\"age\") > 18)\t\\\n",
    "\t.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Stream to Standard Output (for debugging)\n",
    "Foreach Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process each batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Convert the Spark DataFrame to a Pandas DataFrame for easier printing\n",
    "    pandas_df = batch_df.toPandas()\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    print(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use foreachBatch to apply the function to each micro-batch\n",
    "# Foreach Sink supports all output modes: append / complete / update\n",
    "# change the outputMode to see the differences\n",
    "\n",
    "query_01 = df.filter(col(\"age\") > 18).writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Stream to Memory Sink (for debugging)\n",
    "Memory Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Sink supports all output modes: append / complete / update\n",
    "# change the outputMode to see the differences\n",
    "\n",
    "query_02 = df.writeStream.format(\"memory\").outputMode(\"update\").queryName(\"tmp_streaming_tbl\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from tmp_streaming_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Stream to a CSV-formatted table\n",
    "File Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_table_appended = 'notebook_data/w09/data/output/csv_table_appended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_03 = df.writeStream.format('csv').option('header', 'true').outputMode('append').start(csv_table_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_table = 'notebook_data/w09/data/output/csv_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will raise error: AnalysisException: Data source csv does not support Complete output mode.\n",
    "df.writeStream.format('csv').option('header', 'true').outputMode('complete').start(csv_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will raise error: AnalysisException: Data source csv does not support Update output mode.\n",
    "df.writeStream.format('csv').option('header', 'true').outputMode('update').start(csv_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify CSV table using batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv(path = csv_table_appended, inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Stream to a Parquet table\n",
    "File Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table = 'notebook_data/w09/data/output/parquet_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will raise error: AnalysisException: Data source parquet does not support Complete output mode.\n",
    "df.writeStream.format('parquet').outputMode('complete').start(parquet_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will raise error: AnalysisException: Data source parquet does not support Update output mode.\n",
    "df.writeStream.format('parquet').outputMode('update').start(parquet_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, the following should work:\n",
    "query_04 = df.writeStream.format('parquet').outputMode('append').start(parquet_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify data from the Parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(parquet_table).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Stream to a new Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = \"notebook_data/w09/data/output/delta_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write the result to a new Delta Table\n",
    "query_05 = df.writeStream.format(\"delta\").outputMode(\"append\").start(delta_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Delta table using batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the data from Delta table\n",
    "df_delta = spark.read.format('delta').load(delta_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Delta table using Streaming Mode (Spark Structured Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta_streaming = spark.readStream.format('delta').load(delta_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_06 = df_delta_streaming.writeStream.foreachBatch(process_batch).outputMode('append').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_01.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_02.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_03.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_04.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_05.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_06.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Checkpoints & Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the path to the checkpoint directory\n",
    "checkpoint_dir = \"notebook_data/w09/checkpoints/example_01\"\n",
    "output_dir = \"notebook_data/w09/data/output/\"\n",
    "directories = [checkpoint_dir, output_dir]\n",
    "\n",
    "# Remove the checkpoint directory if it exists\n",
    "for dir in directories:\n",
    "\tif os.path.exists(dir):\n",
    "\t\tshutil.rmtree(dir)\n",
    "\t\tprint(f\"Directory {dir} cleaned up.\")\n",
    "\telse:\n",
    "\t\tprint(f\"Directory {dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
