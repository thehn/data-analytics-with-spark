{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common rules\n",
    "1. DO NOT change the variables names which are formatted as `df_xx`\n",
    "2. You can display the data using the `.show()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: Read CSV File\n",
    "1. Read the CSV file into a DataFrame namely `df`\n",
    "2. Print the schema of DataFrame `df`.\n",
    "3. Cache this `df` into memory for reuses below.\n",
    "4. Get the first 5 rows of the DataFrame `df` to driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# the following Spark session creation contains jars which support writing to Delta format\n",
    "# note: checkout this link to see which version of Delta is compatible with your version of Pyspark: https://docs.delta.io/latest/releases.html\n",
    "spark = SparkSession.builder\t\\\n",
    "\t.config('spark.jars.packages', 'io.delta:delta-spark_2.12:3.2.0')\t\\\n",
    "\t.config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\t\\\n",
    "\t.config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\t\\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('header', 'true').load('exams/data/sample.csv')\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df = df.cache()\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2: Count Rows\n",
    "Count the number of rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = df.count()\n",
    "print(\"Number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 3: Filter Rows\n",
    "Filter rows where the column 'age' is greater than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_03 = df.filter('age > 30')\n",
    "df_03.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 4: Select Columns\n",
    "Select only the columns 'name' and 'age'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_04 = df.select('name', 'age')\n",
    "df_04.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 5: Group By and Aggregate\n",
    "Group by 'department' and calculate the average 'salary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df_05 = df.groupBy('department').agg(avg('salary'))\n",
    "df_05.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6: Rename Column\n",
    "Rename the column 'name' to 'new_name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_06 = df.withColumnRenamed('name', 'new_name')\n",
    "df_06.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 7: Drop Column\n",
    "Drop the column 'salary' from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_07 = df.drop('salary')\n",
    "df_07.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 8: Add New Column\n",
    "Add a new column 'bonus' which is 10% of 'salary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_08 = df.withColumn('bonus', 0.1 * col('salary'))\n",
    "\n",
    "df_08.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 9: Sort DataFrame\n",
    "Sort the DataFrame by 'age' in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df_09 = df.orderBy(desc('age'))\n",
    "df_09.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 10: Write DataFrame to Parquet\n",
    "Write the DataFrame to a Parquet file named \"output/data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('exams/12345678/output/data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 11: Read JSON File\n",
    "Read a JSON file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11 = spark.read.option('multiLine', 'true').json('exams/data/sample.json')\n",
    "df_11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 12: Drop Duplicates\n",
    "Drop duplicate rows based on the 'name' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12 = df.dropDuplicates(['name'])\n",
    "df_12.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 13: Check for Null Values\n",
    "1. Check for null values in the 'email' column.\n",
    "2. Drop the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_13_1 = df.filter(col('email').isNull())\n",
    "df_13_1.show()\n",
    "\n",
    "df_13_2 = df.dropna(subset = ['email'])\n",
    "df_13_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 14: Create Temporary View\n",
    "Create a temporary view named 'people_view' from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people_view')\n",
    "df_14 = spark.sql(\"SELECT * FROM people_view\")\n",
    "df_14.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 15: SQL Query on DataFrame\n",
    "Execute an SQL query to select names of people older than 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_15 = spark.sql('select * from people_view where age > 25')\n",
    "df_15.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 16: Join Two DataFrames using Spark SQL\n",
    "1. Loading data from `exams/data/department.csv` into a temporary view named `department`\n",
    "2. Join two DataFrames on the 'id' column to find names of department managers of each department. Choose the join type which eliminate the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = spark.read.option('header', 'true').csv('exams/data/department.csv').cache()\n",
    "\n",
    "tmp_df.createOrReplaceTempView('department')\n",
    "df_16 = spark.sql('select d.*, p.name from department d inner join people_view p on d.manager_id = p.id')\n",
    "df_16.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 17: Save DataFrame as Delta Table\n",
    "Save the DataFrame as a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").save(\"exams/output/delta_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 18: Read Delta Table\n",
    "Read a Delta table into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18 = spark.read.format('delta').load('exams/output/delta_table')\n",
    "df_18.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 19: Use UDF (User Defined Function)\n",
    "Define a UDF to convert names to uppercase and apply it to the `name` column. Name the udf-applied column as `upper_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "upper_udf = udf(lambda x: x.upper())\n",
    "df_19 = df.select(upper_udf(df.name).alias('upper_name'))\n",
    "df_19.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 20: Extract Year from Date Column\n",
    "Extract the year from a date column 'hire_date' as a new column named 'hire_year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "df_20 = df.select(year(df.hire_date).alias(\"hire_year\"))\n",
    "df_20.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 21: Create a New DataFrame from Existing Columns\n",
    "Create a new DataFrame with only 'name', 'age', and 'salary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_21 = df.select('name', 'age', 'salary')\n",
    "df_21.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 22: Pivot Table\n",
    "Create a pivot table to show total salary by department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# first, cast the salary column to double type\n",
    "df = df.withColumn('salary', col('salary').cast('int'))\n",
    "df_22 = df.groupBy(\"department\").pivot(\"gender\").sum('salary').alias('total_salary')\n",
    "df_22.drop('null').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 23: Handle Missing Values\n",
    "Fill missing values in the 'salary' column with the average salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "avg_salary = df.select(avg('salary')).collect()[0][0]\n",
    "print(avg_salary)\n",
    "df_23 = df.fillna(value=avg_salary, subset=['salary'])\n",
    "df_23.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 24: Convert String to Date Type\n",
    "Convert the string column 'hire_date' to DateType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df_24 = df.withColumn('hire_date', to_date('hire_date', 'yyyy-MM-dd'))\n",
    "df_24.printSchema()\n",
    "df_24.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 25: Count Distinct Values\n",
    "Count distinct values in the 'department' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_count = df.select('department').distinct().count()\n",
    "print(distinct_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 26: Find Maximum Value in a Column\n",
    "Find the maximum value in the 'salary' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df_26 = df.select(max('salary'))\n",
    "df_26.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 27: Use Window Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(df.salary.desc())\n",
    "df_27 = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "df_27.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 28: Save DataFrame as CSV with Options\n",
    "Save the DataFrame as a CSV file with header and compression options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\", \"true\").option(\"compression\", \"gzip\").csv(\"exams/output/compressed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 29: Read Multiple Files into a Single DataFrame\n",
    "Read multiple CSV files from a directory into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_29 = spark.read.csv(\"exams/data/*.csv\", header=True)\n",
    "df_29.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
